{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "369e80f5-0bbe-41c3-aa4c-6d1a34ff212b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\OneDrive\\Documents\\MAIA\\Semestre 4\\Despliegue de Soluciones\\__REPOS__\\Clasificacion-de-textos-medicos\\.venv_clasificador\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2025/10/13 23:15:35 INFO mlflow.tracking.fluent: Experiment with name '/Desarrollo_de_Soluciones/Combined_Datasets' does not exist. Creating a new experiment.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Experiment: artifact_location='dbfs:/databricks/mlflow-tracking/4041722711137697', creation_time=1760390133937, experiment_id='4041722711137697', last_update_time=1760390133937, lifecycle_stage='active', name='/Desarrollo_de_Soluciones/Combined_Datasets', tags={'mlflow.experiment.sourceName': '/Desarrollo_de_Soluciones/Combined_Datasets',\n",
       " 'mlflow.experimentType': 'MLFLOW_EXPERIMENT',\n",
       " 'mlflow.ownerEmail': 'maiauniandes@gmail.com',\n",
       " 'mlflow.ownerId': '78764819635503'}>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from itertools import product\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import average_precision_score, roc_auc_score, f1_score, recall_score, accuracy_score\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from tqdm.auto import tqdm\n",
    "import math\n",
    "import mlflow\n",
    "import os\n",
    "\n",
    "os.environ[\"DATABRICKS_HOST\"] = \"https://dbc-2d843358-2bd3.cloud.databricks.com/\"\n",
    "os.environ[\"DATABRICKS_TOKEN\"] = \"dapid34b343fd31d5e3c797e5d9d6966dcf2\"\n",
    "mlflow.set_tracking_uri(\"databricks\")\n",
    "mlflow.set_experiment(\"/Desarrollo_de_Soluciones/Combined_Datasets\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "710acec1-17eb-4613-a1f9-7e070d602d34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cargar los datos y procesar los datos\n",
    "\n",
    "data_training = pd.read_csv(\"../data/cleaned_train_dataset.csv\")\n",
    "data_test = pd.read_csv(\"../data/cleaned_test_dataset.csv\")\n",
    "data_validation = pd.read_csv(\"../data/cleaned_val_dataset.csv\")\n",
    "data_coachrane = pd.read_csv(\"../data/cochrane_sample_large.csv\")\n",
    "data_coachrane[\"label\"] = (data_coachrane[\"label\"].str.strip().str.lower().map({\"plain\": 0, \"technical\": 1}).astype(\"int8\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "970c8e17-6264-4fcc-a8d7-c350ee07dfbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combinar los DF\n",
    "frames = []\n",
    "for df in (data_training, data_validation, data_test):\n",
    "    frames.append(df[[\"text\", \"label\"]])\n",
    "frames.append(data_coachrane[[\"text\", \"label\"]])\n",
    "\n",
    "data_all = pd.concat(frames, ignore_index=True).dropna(subset=[\"text\", \"label\"])\n",
    "data_all[\"label\"] = data_all[\"label\"].astype(int)\n",
    "\n",
    "train_df, val_df = train_test_split(data_all, test_size=0.2, shuffle=True, random_state=42, stratify=data_all[\"label\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bac29f82-f22e-40fe-a735-d384e204a41c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hiperparametros de los Modelos y vectorizador\n",
    "\n",
    "tfidf_params = {\n",
    "    \"ngram_range\": (1, 2),\n",
    "    \"min_df\": 0.01,\n",
    "    \"max_features\": 10000,\n",
    "}\n",
    "\n",
    "logreg_params = {\n",
    "    \"C\": 1.0,            \n",
    "    \"solver\": \"liblinear\", \n",
    "}\n",
    "\n",
    "xgb_params = {\n",
    "    \"learning_rate\": 0.1,\n",
    "    \"n_estimators\": 400,\n",
    "    \"max_depth\": 4,\n",
    "}\n",
    "\n",
    "nb_params = {\n",
    "    \"alpha\": 0.1,\n",
    "}\n",
    "\n",
    "svm_params = {\n",
    "    \"kernel\": \"linear\",\n",
    "    \"C\": 1.0,\n",
    "    \"probability\": True,\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "736c341d-3373-47aa-b09d-c7949a0fa8cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crear vectores a partir de los datos\n",
    "\n",
    "vec = TfidfVectorizer(**tfidf_params)\n",
    "X_train = vec.fit_transform(train_df[\"text\"].astype(str))\n",
    "X_val   = vec.transform(val_df[\"text\"].astype(str))\n",
    "\n",
    "y_train = train_df[\"label\"].values\n",
    "y_val   = val_df[\"label\"].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d6e15cf9-4b28-47c4-9690-ead8463868af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tamanio de la data vectorizada: 1862 muestras de 2693 dimensiones\n"
     ]
    }
   ],
   "source": [
    "print(f\"tamanio de la data vectorizada: {X_train.shape[0]} muestras de {X_train.shape[1]} dimensiones\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0e5143ee-632e-4472-a707-454e0d628a10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modelos\n",
    "\n",
    "logreg = LogisticRegression(**logreg_params)\n",
    "\n",
    "xgboost = XGBClassifier(\n",
    "    objective=\"binary:logistic\",\n",
    "    **xgb_params\n",
    ")\n",
    "\n",
    "naive_bayes = MultinomialNB(alpha=nb_params[\"alpha\"])\n",
    "\n",
    "svm = SVC(**svm_params)\n",
    "\n",
    "modelos = [logreg, xgboost, naive_bayes, svm]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "78aaafa4-d093-48c3-baa4-581b88ad57aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_scores_and_preds(model, X):\n",
    "    if hasattr(model, \"predict_proba\"):\n",
    "        s = model.predict_proba(X)[:, 1]\n",
    "        p = (s >= 0.5).astype(int)\n",
    "    else:\n",
    "        s = model.decision_function(X)\n",
    "        p = (s >= 0).astype(int)\n",
    "    return s, p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c0f79b98-7684-427c-8fe7-8214a6e11236",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LogisticRegression:   0%|          | 0/4 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🏃 View run LogisticRegression at: https://dbc-2d843358-2bd3.cloud.databricks.com/ml/experiments/4041722711137697/runs/d7758cb58db7423596487c0621ab4bb8\n",
      "🧪 View experiment at: https://dbc-2d843358-2bd3.cloud.databricks.com/ml/experiments/4041722711137697\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "MultinomialNB:  50%|█████     | 2/4 [00:06<00:06,  3.36s/it]     "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🏃 View run XGBClassifier at: https://dbc-2d843358-2bd3.cloud.databricks.com/ml/experiments/4041722711137697/runs/ac81afde8dfe4fad92aa3b9ff3608336\n",
      "🧪 View experiment at: https://dbc-2d843358-2bd3.cloud.databricks.com/ml/experiments/4041722711137697\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "SVC:  75%|███████▌  | 3/4 [00:08<00:02,  2.92s/it]          "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🏃 View run MultinomialNB at: https://dbc-2d843358-2bd3.cloud.databricks.com/ml/experiments/4041722711137697/runs/1bbc5ae7ecc1469d8cb57863faed595b\n",
      "🧪 View experiment at: https://dbc-2d843358-2bd3.cloud.databricks.com/ml/experiments/4041722711137697\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "SVC: 100%|██████████| 4/4 [00:14<00:00,  3.50s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🏃 View run SVC at: https://dbc-2d843358-2bd3.cloud.databricks.com/ml/experiments/4041722711137697/runs/2f44a83c392c4d01921594f03fcff747\n",
      "🧪 View experiment at: https://dbc-2d843358-2bd3.cloud.databricks.com/ml/experiments/4041722711137697\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "pbar = tqdm(modelos, desc=\"Combined eval\", leave=True)\n",
    "for model in pbar:\n",
    "    run_name = f\"{model.__class__.__name__}\"\n",
    "    pbar.set_description(run_name)\n",
    "\n",
    "    with mlflow.start_run(run_name=run_name):\n",
    "        mlflow.set_tag(\"dataset\", \"combined_old+cochrane\")\n",
    "        mlflow.log_param(\"n_train\", int(X_train.shape[0]))\n",
    "        mlflow.log_param(\"n_val\", int(X_val.shape[0]))\n",
    "        mlflow.log_param(\"vocab_size\", int(X_train.shape[1]))\n",
    "        mlflow.log_param(\"tfidf_ngram_range\", str(tfidf_params.get(\"ngram_range\")))\n",
    "        mlflow.log_param(\"tfidf_min_df\", tfidf_params.get(\"min_df\"))\n",
    "        mlflow.log_param(\"tfidf_max_features\", tfidf_params.get(\"max_features\"))\n",
    "\n",
    "        # Entrena\n",
    "        model.fit(X_train, y_train)\n",
    "\n",
    "        # Val\n",
    "        s_val, yhat_val = get_scores_and_preds(model, X_val)\n",
    "        metrics_val = {\n",
    "            \"val_pr_auc\": float(average_precision_score(y_val, s_val)),\n",
    "            \"val_roc_auc\": float(roc_auc_score(y_val, s_val)),\n",
    "            \"val_f1\": float(f1_score(y_val, yhat_val)),\n",
    "            \"val_recall\": float(recall_score(y_val, yhat_val)),\n",
    "            \"val_accuracy\": float(accuracy_score(y_val, yhat_val)),\n",
    "        }\n",
    "        mlflow.log_metrics(metrics_val)\n",
    "\n",
    "pbar.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cddca4ff-ad98-4965-8809-05f47180624a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Experimento Concluido con Exito!\n"
     ]
    }
   ],
   "source": [
    "print(\"Experimento Concluido con Exito!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4051f743-7d74-4aca-b5b7-c4ba1e3de730",
   "metadata": {},
   "outputs": [],
   "source": [
    "#texto= \"The initial symptoms were similar to other viral diseases that are still extant, such as influenza and the common cold: fever of at least 38.3 °C (101 °F), muscle pain, malaise, headache and fatigue. As the digestive tract was commonly involved, nausea, vomiting, and backache often occurred. The early prodromal stage usually lasted 2–4 days. By days 12–15, the first visible lesions – small reddish spots called enanthem – appeared on mucous membranes of the mouth, tongue, palate, and throat, and the temperature fell to near-normal. These lesions rapidly enlarged and ruptured, releasing large amounts of virus into the saliva.\"\n",
    "texto = \"In contrast to earlier views on BPD, this condition can remit, and symptoms can be reduced and managed. Nevertheless, specific symptoms such as fear of abandonment, impulsivity, intense anger, and an unstable self-image may persist. Individuals with BPD may also continue to experience impairments in social and occupational functioning and may have a need for ongoing treatment. Rates of suicide attempts and episodes of self-harm also decline over time, but they continue to occur more often than in individuals without BPD. Furthermore, in longitudinal studies, BPD is associated with increases in deaths due to suicide as well as with all-cause mortality. Thus, the lifetime burden and psychosocial impairment associated with BPD can be substantial because it typically has an onset in adolescence or early adulthood and can persist for many years. In addition, individuals with BPD experience increases in health care costs related to BPD and to other physical conditions.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "05aaf1a2-e272-4f94-841b-7e79aee5f676",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "segun LogisticRegression el texto es technical\n",
      "segun XGBClassifier el texto es technical\n",
      "segun MultinomialNB el texto es technical\n",
      "segun SVC el texto es technical\n"
     ]
    }
   ],
   "source": [
    "vector = vec.transform([texto.lower()])\n",
    "preds = [clf.predict(vector) for clf in modelos]\n",
    "\n",
    "inv_map = {0: \"plain\", 1: \"technical\"}\n",
    "for clf, pred in zip(modelos, preds):\n",
    "    label = int(pred[0])      # o: pred.item()\n",
    "    print(f\"segun {clf.__class__.__name__} el texto es {inv_map[label]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e0503485-98ef-43a7-8039-926a7ad0d79e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running bdist_wheel\n",
      "running build\n",
      "running build_py\n",
      "creating build\n",
      "creating build\\lib\n",
      "creating build\\lib\\textclf_svm\n",
      "copying textclf_svm\\__init__.py -> build\\lib\\textclf_svm\n",
      "running egg_info\n",
      "creating textclf_svm.egg-info\n",
      "writing textclf_svm.egg-info\\PKG-INFO\n",
      "writing dependency_links to textclf_svm.egg-info\\dependency_links.txt\n",
      "writing requirements to textclf_svm.egg-info\\requires.txt\n",
      "writing top-level names to textclf_svm.egg-info\\top_level.txt\n",
      "writing manifest file 'textclf_svm.egg-info\\SOURCES.txt'\n",
      "reading manifest file 'textclf_svm.egg-info\\SOURCES.txt'\n",
      "reading manifest template 'MANIFEST.in'\n",
      "writing manifest file 'textclf_svm.egg-info\\SOURCES.txt'\n",
      "copying textclf_svm\\metrics.json -> build\\lib\\textclf_svm\n",
      "copying textclf_svm\\model.joblib -> build\\lib\\textclf_svm\n",
      "copying textclf_svm\\tfidf.joblib -> build\\lib\\textclf_svm\n",
      "installing to build\\bdist.win-amd64\\wheel\n",
      "running install\n",
      "running install_lib\n",
      "creating build\\bdist.win-amd64\n",
      "creating build\\bdist.win-amd64\\wheel\n",
      "creating build\\bdist.win-amd64\\wheel\\textclf_svm\n",
      "copying build\\lib\\textclf_svm\\metrics.json -> build\\bdist.win-amd64\\wheel\\.\\textclf_svm\n",
      "copying build\\lib\\textclf_svm\\model.joblib -> build\\bdist.win-amd64\\wheel\\.\\textclf_svm\n",
      "copying build\\lib\\textclf_svm\\tfidf.joblib -> build\\bdist.win-amd64\\wheel\\.\\textclf_svm\n",
      "copying build\\lib\\textclf_svm\\__init__.py -> build\\bdist.win-amd64\\wheel\\.\\textclf_svm\n",
      "running install_egg_info\n",
      "Copying textclf_svm.egg-info to build\\bdist.win-amd64\\wheel\\.\\textclf_svm-0.1.0-py3.11.egg-info\n",
      "running install_scripts\n",
      "creating build\\bdist.win-amd64\\wheel\\textclf_svm-0.1.0.dist-info\\WHEEL\n",
      "creating 'wheel_svm\\dist\\textclf_svm-0.1.0-py3-none-any.whl' and adding 'build\\bdist.win-amd64\\wheel' to it\n",
      "adding 'textclf_svm/__init__.py'\n",
      "adding 'textclf_svm/metrics.json'\n",
      "adding 'textclf_svm/model.joblib'\n",
      "adding 'textclf_svm/tfidf.joblib'\n",
      "adding 'textclf_svm-0.1.0.dist-info/METADATA'\n",
      "adding 'textclf_svm-0.1.0.dist-info/WHEEL'\n",
      "adding 'textclf_svm-0.1.0.dist-info/top_level.txt'\n",
      "adding 'textclf_svm-0.1.0.dist-info/RECORD'\n",
      "removing build\\bdist.win-amd64\\wheel\n",
      "\n",
      "Wheel generado: D:\\OneDrive\\Documents\\MAIA\\Semestre 4\\Despliegue de Soluciones\\__REPOS__\\Clasificacion-de-textos-medicos\\EXPLORACION\\wheel_svm\\wheel_svm\\dist\\textclf_svm-0.1.0-py3-none-any.whl\n"
     ]
    }
   ],
   "source": [
    "# === Build de un wheel mínimo para UN modelo ya entrenado (SVM) ===\n",
    "# Reqs: pip install wheel setuptools joblib\n",
    "import sys, shutil, subprocess, textwrap, json\n",
    "import os, stat, time, uuid, shutil\n",
    "from pathlib import Path\n",
    "from pathlib import Path\n",
    "import joblib\n",
    "import numpy as np\n",
    "from sklearn.metrics import average_precision_score, roc_auc_score, f1_score, recall_score, accuracy_score\n",
    "\n",
    "# --- Selecciona el modelo ya ENTRENADO y el vectorizador TF-IDF ---\n",
    "pkg_key   = \"svm\"       # puedes cambiar a \"naive_bayes\"/\"xgboost/logreg\" \n",
    "model_obj = svm         # objeto del modelo ya entrenado\n",
    "vec_obj   = vec         # TfidfVectorizer ya entrenado\n",
    "\n",
    "# --- Estructura del paquete ---\n",
    "\n",
    "\n",
    "\n",
    "def _rm_readonly_and_retry(func, path, excinfo):\n",
    "    \"\"\"Permite borrar archivos bloqueados (Windows).\"\"\"\n",
    "    try:\n",
    "        os.chmod(path, stat.S_IWRITE)\n",
    "    except Exception:\n",
    "        pass\n",
    "    time.sleep(0.1)\n",
    "    func(path)\n",
    "\n",
    "root = Path(f\"wheel_{pkg_key}\")\n",
    "if root.exists():\n",
    "    try:\n",
    "        shutil.rmtree(root, onerror=_rm_readonly_and_retry)\n",
    "    except PermissionError:\n",
    "        # Si sigue bloqueado (por ejemplo, import abierto o indexado)\n",
    "        trash = root.with_name(root.name + f\".old_{uuid.uuid4().hex}\")\n",
    "        print(f\"[WARN] No se pudo borrar {root}, renombrando a {trash}\")\n",
    "        root.rename(trash)\n",
    "\n",
    "pkg_name = f\"textclf_{pkg_key}\"\n",
    "pkg_dir  = root / pkg_name\n",
    "dist_dir = root / \"dist\"\n",
    "\n",
    "pkg_dir.mkdir(parents=True, exist_ok=True)\n",
    "dist_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "\n",
    "# --- Artefactos del modelo ---\n",
    "joblib.dump(vec_obj,   pkg_dir / \"tfidf.joblib\")\n",
    "joblib.dump(model_obj, pkg_dir / \"model.joblib\")\n",
    "\n",
    "# --- Empaquetar métricas del entrenamiento/validación (constantes en el wheel) ---\n",
    "def _scores_for(mdl, X):\n",
    "    if hasattr(mdl, \"predict_proba\"):\n",
    "        return mdl.predict_proba(X)[:, 1]\n",
    "    elif hasattr(mdl, \"decision_function\"):\n",
    "        from scipy.special import expit\n",
    "        return expit(mdl.decision_function(X))\n",
    "    else:\n",
    "        return np.full(X.shape[0], 0.5, dtype=float)\n",
    "\n",
    "\n",
    "X_val_pkg = vec_obj.transform(val_df[\"text\"].astype(str))\n",
    "s_val_pkg = _scores_for(model_obj, X_val_pkg)\n",
    "y_pred_pkg = (s_val_pkg >= 0.5).astype(int)\n",
    "\n",
    "metrics_val_pkg = {\n",
    "    \"pr_auc\":   float(average_precision_score(y_val, s_val_pkg)),\n",
    "    \"roc_auc\":  float(roc_auc_score(y_val, s_val_pkg)),\n",
    "    \"f1\":       float(f1_score(y_val, y_pred_pkg)),\n",
    "    \"recall\":   float(recall_score(y_val, y_pred_pkg)),\n",
    "    \"accuracy\": float(accuracy_score(y_val, y_pred_pkg)),\n",
    "    \"threshold\": 0.5,\n",
    "    \"n_val\": int(X_val_pkg.shape[0]),\n",
    "}\n",
    "\n",
    "(pkg_dir / \"metrics.json\").write_text(json.dumps(metrics_val_pkg, ensure_ascii=False, indent=2), encoding=\"utf-8\")\n",
    "\n",
    "# --- __init__.py con API: predict() y metrics() (sin args) ---\n",
    "init_code = \"\"\"\n",
    "import json\n",
    "import joblib\n",
    "from importlib.resources import files\n",
    "import numpy as np\n",
    "\n",
    "_VEC = joblib.load(files(__package__) / \"tfidf.joblib\")\n",
    "_MDL = joblib.load(files(__package__) / \"model.joblib\")\n",
    "_METRICS = json.loads((files(__package__) / \"metrics.json\").read_text(encoding=\"utf-8\"))\n",
    "\n",
    "def _scores_for(X):\n",
    "    if hasattr(_MDL, \"predict_proba\"):\n",
    "        return _MDL.predict_proba(X)[:, 1]\n",
    "    elif hasattr(_MDL, \"decision_function\"):\n",
    "        from scipy.special import expit\n",
    "        return expit(_MDL.decision_function(X))\n",
    "    else:\n",
    "        return np.full(X.shape[0], 0.5, dtype=float)\n",
    "\n",
    "def predict(texts, threshold=0.5):\n",
    "    X = _VEC.transform([str(t) for t in texts])\n",
    "    s = _scores_for(X)\n",
    "    y = (s >= float(threshold)).astype(int).tolist()\n",
    "    return {\"labels\": y, \"scores\": [float(v) for v in s]}\n",
    "\n",
    "def metrics():\n",
    "    \\\"\\\"\\\"Devuelve las métricas calculadas en validación durante el entrenamiento.\\\"\\\"\\\"\n",
    "    return dict(_METRICS)\n",
    "\"\"\".strip()\n",
    "(pkg_dir / \"__init__.py\").write_text(init_code, encoding=\"utf-8\")\n",
    "\n",
    "# --- setup.cfg + setup.py mínimos ---\n",
    "setup_cfg = f\"\"\"\n",
    "[metadata]\n",
    "name = {pkg_name}\n",
    "version = 0.1.0\n",
    "description = Minimal wheel with TF-IDF + {pkg_key} model\n",
    "\n",
    "[options]\n",
    "packages = find:\n",
    "include_package_data = True\n",
    "install_requires =\n",
    "    scikit-learn\n",
    "    joblib\n",
    "    numpy\n",
    "    scipy\n",
    "python_requires = >=3.9\n",
    "\n",
    "[options.package_data]\n",
    "{pkg_name} = *.joblib, *.json\n",
    "\"\"\"\n",
    "(root / \"setup.cfg\").write_text(textwrap.dedent(setup_cfg).strip(), encoding=\"utf-8\")\n",
    "(root / \"setup.py\").write_text(\"from setuptools import setup; setup()\", encoding=\"utf-8\")\n",
    "(root / \"MANIFEST.in\").write_text(f\"recursive-include {pkg_name} *.joblib *.json\\n\", encoding=\"utf-8\")\n",
    "\n",
    "# --- Asegura herramientas de build ---\n",
    "subprocess.run([sys.executable, \"-m\", \"pip\", \"install\", \"-q\", \"wheel\", \"setuptools>=61.0\"], check=True)\n",
    "\n",
    "# --- Construir wheel ---\n",
    "proc = subprocess.run(\n",
    "    [sys.executable, \"setup.py\", \"bdist_wheel\", \"-d\", str(dist_dir)],\n",
    "    cwd=str(root),\n",
    "    text=True,\n",
    "    capture_output=True\n",
    ")\n",
    "print(proc.stdout)\n",
    "if proc.returncode != 0:\n",
    "    print(\"=== STDERR ===\")\n",
    "    print(proc.stderr)\n",
    "    raise RuntimeError(\"Fallo al construir el wheel\")\n",
    "\n",
    "# --- Localizar el wheel ---\n",
    "wheels = list(dist_dir.glob(\"*.whl\")) or list(root.rglob(\"*.whl\"))\n",
    "if not wheels:\n",
    "    raise RuntimeError(\"No se generó ningún .whl; revisa el log arriba.\")\n",
    "print(\"Wheel generado:\", wheels[0].resolve())\n",
    "# Instálalo con: pip install <ruta_mostrada>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8fc4999f-70bc-4d3e-85eb-580253bb5da5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Markdown\n",
    "\n",
    "def export_code_cells():\n",
    "    from IPython import get_ipython\n",
    "    cells = get_ipython().user_ns['In']\n",
    "    code = '\\n\\n'.join([c for c in cells if c.strip()])\n",
    "    return Markdown(f'```python\\n{code}\\n```')\n",
    "\n",
    "#export_code_cells()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d27bc504-fda9-47b3-8a29-8e1d198e7ef5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv_clasificador",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
