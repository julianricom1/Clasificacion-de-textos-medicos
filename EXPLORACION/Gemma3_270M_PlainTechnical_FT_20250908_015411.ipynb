{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "008cc2cc",
   "metadata": {},
   "source": [
    "# Fine‑tuning Gemma 3 270M (IT) for Plain vs Technical Classification\n",
    "\n",
    "Este cuaderno está listo para **Google Colab**. Cubre:\n",
    "- Login a Hugging Face (gated access) y smoke test del repo.\n",
    "- Carga de CSVs (`text`, `label`), tokenización y `DataCollatorWithPadding`.\n",
    "- Modelo `Gemma3TextForSequenceClassification` para clasificación.\n",
    "- Entrenamiento, evaluación (métricas + matriz de confusión).\n",
    "- Guardado/recarga e inferencia con softmax.\n",
    "- (Opcional) push al Hub privado.\n",
    "\n",
    "**Requisitos previos**: Aceptar la licencia de `google/gemma-3-270m-it` en Hugging Face y tener token."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d826354",
   "metadata": {},
   "source": [
    "## 1) Instalación de dependencias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84aceb90",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip -q uninstall -y transformers >/dev/null\n",
    "!pip -q install --no-deps -U git+https://github.com/huggingface/transformers.git >/dev/null\n",
    "!pip -q install -U huggingface_hub datasets accelerate evaluate scikit-learn sentencepiece bitsandbytes matplotlib >/dev/null\n",
    "\n",
    "import transformers, datasets, huggingface_hub\n",
    "print('Transformers:', transformers.__version__)\n",
    "print('Datasets:', datasets.__version__)\n",
    "print('HF Hub:', huggingface_hub.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee039fe9",
   "metadata": {},
   "source": [
    "## 2) Login a Hugging Face (gated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ee8c579",
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import login, whoami, hf_hub_download\n",
    "from getpass import getpass\n",
    "\n",
    "HF_TOKEN = getpass('Pega tu token de Hugging Face (empieza por hf_): ').strip()\n",
    "login(HF_TOKEN)\n",
    "print('Usuario:', whoami().get('name'))\n",
    "\n",
    "_ = hf_hub_download(repo_id='google/gemma-3-270m-it', filename='config.json', token=HF_TOKEN)\n",
    "print('OK: acceso a google/gemma-3-270m-it')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdd241cb",
   "metadata": {},
   "source": [
    "## 3) Imports y utilidades"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bbc56c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, numpy as np, pandas as pd, torch\n",
    "import matplotlib.pyplot as plt\n",
    "from datasets import Dataset, DatasetDict, Value\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support, confusion_matrix, classification_report, roc_auc_score\n",
    "from transformers import AutoTokenizer, AutoConfig, DataCollatorWithPadding, TrainingArguments, Trainer\n",
    "try:\n",
    "    from transformers import Gemma3TextForSequenceClassification\n",
    "except Exception:\n",
    "    from transformers.models.gemma3.modeling_gemma3 import Gemma3TextForSequenceClassification\n",
    "\n",
    "SEED = 42\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b163647e",
   "metadata": {},
   "source": [
    "## 4) Carga de datos (CSV con columnas `text`, `label`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35c6c373",
   "metadata": {},
   "outputs": [],
   "source": [
    "# EDITA LAS RUTAS A TUS CSVs\n",
    "TRAIN_CSV = '/content/train.csv'\n",
    "VAL_CSV   = '/content/val.csv'\n",
    "TEST_CSV  = '/content/test.csv'\n",
    "\n",
    "train_df = pd.read_csv(TRAIN_CSV)\n",
    "val_df   = pd.read_csv(VAL_CSV)\n",
    "test_df  = pd.read_csv(TEST_CSV)\n",
    "\n",
    "assert 'text' in train_df.columns and 'label' in train_df.columns\n",
    "assert 'text' in val_df.columns and 'label' in val_df.columns\n",
    "assert 'text' in test_df.columns and 'label' in test_df.columns\n",
    "print('train:', train_df.shape, '| val:', val_df.shape, '| test:', test_df.shape)\n",
    "train_df.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e10e8ad",
   "metadata": {},
   "source": [
    "## 5) Tokenizador y preparación de `DatasetDict`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a41ec193",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_id = 'google/gemma-3-270m-it'\n",
    "num_labels = 2\n",
    "id2label = {0: 'plain', 1: 'technical'}\n",
    "label2id = {'plain': 0, 'technical': 1}\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id, token=HF_TOKEN, use_fast=True)\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "def ensure_int_labels(df):\n",
    "    if df['label'].dtype == 'object':\n",
    "        df['label'] = df['label'].map(label2id)\n",
    "    df['label'] = df['label'].astype('int64')\n",
    "    return df\n",
    "\n",
    "train_df = ensure_int_labels(train_df)\n",
    "val_df   = ensure_int_labels(val_df)\n",
    "test_df  = ensure_int_labels(test_df)\n",
    "\n",
    "train_ds = Dataset.from_pandas(train_df, preserve_index=False).cast_column('label', Value('int64'))\n",
    "val_ds   = Dataset.from_pandas(val_df,   preserve_index=False).cast_column('label', Value('int64'))\n",
    "test_ds  = Dataset.from_pandas(test_df,  preserve_index=False).cast_column('label', Value('int64'))\n",
    "\n",
    "def tokenize_function(batch):\n",
    "    return tokenizer(batch['text'], truncation=True)\n",
    "\n",
    "keep_cols = ['text', 'label']\n",
    "rm_train = [c for c in train_ds.column_names if c not in keep_cols]\n",
    "rm_val   = [c for c in val_ds.column_names   if c not in keep_cols]\n",
    "rm_test  = [c for c in test_ds.column_names  if c not in keep_cols]\n",
    "\n",
    "tokenized = DatasetDict({\n",
    "    'train': train_ds.map(tokenize_function, batched=True, remove_columns=rm_train),\n",
    "    'validation': val_ds.map(tokenize_function, batched=True, remove_columns=rm_val),\n",
    "    'test': test_ds.map(tokenize_function, batched=True, remove_columns=rm_test),\n",
    "})\n",
    "\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "print(tokenized)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79e1a02b",
   "metadata": {},
   "source": [
    "## 6) Modelo Gemma3 Text para clasificación"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f135e87a",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = AutoConfig.from_pretrained(model_id, token=HF_TOKEN)\n",
    "config.num_labels = num_labels\n",
    "config.id2label = id2label\n",
    "config.label2id = label2id\n",
    "\n",
    "model = Gemma3TextForSequenceClassification.from_pretrained(\n",
    "    model_id,\n",
    "    token=HF_TOKEN,\n",
    "    config=config,\n",
    ")\n",
    "model.config.pad_token_id = tokenizer.pad_token_id\n",
    "\n",
    "for n, p in model.named_parameters():\n",
    "    if 'score' in n:\n",
    "        print('Capa de clasificación:', n, p.shape)\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e815e721",
   "metadata": {},
   "source": [
    "## 7) Entrenamiento con `Trainer`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c7f3eb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "bf16_flag = bool(torch.cuda.is_available() and torch.cuda.get_device_capability(0)[0] >= 8)\n",
    "fp16_flag = False\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    preds, labels = eval_pred\n",
    "    preds = preds.argmax(-1)\n",
    "    p, r, f1, _ = precision_recall_fscore_support(labels, preds, average='binary', zero_division=0)\n",
    "    acc = accuracy_score(labels, preds)\n",
    "    return {'accuracy': acc, 'precision': p, 'recall': r, 'f1': f1}\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='runs_gemma3_270m_cls',\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.01,\n",
    "    save_strategy='epoch',\n",
    "    logging_steps=50,\n",
    "    load_best_model_at_end=False,\n",
    "    metric_for_best_model='f1',\n",
    "    greater_is_better=True,\n",
    "    report_to='none',\n",
    "    bf16=bf16_flag,\n",
    "    fp16=fp16_flag,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized['train'],\n",
    "    eval_dataset=tokenized['validation'],\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "train_result = trainer.train()\n",
    "metrics_val = trainer.evaluate(tokenized['validation'])\n",
    "metrics_val"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bab463c5",
   "metadata": {},
   "source": [
    "## 8) Guardar y recargar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdfac799",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_dir = 'finetuned_gemma3_270m_cls'\n",
    "trainer.save_model(save_dir)\n",
    "tokenizer.save_pretrained(save_dir)\n",
    "\n",
    "tok_ft = AutoTokenizer.from_pretrained(save_dir, use_fast=True)\n",
    "model_ft = Gemma3TextForSequenceClassification.from_pretrained(save_dir)\n",
    "model_ft.config.pad_token_id = tok_ft.pad_token_id\n",
    "print('Recargado desde:', save_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71c6e267",
   "metadata": {},
   "source": [
    "## 9) Inferencia con softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "395e3051",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "def classify_text(text: str, max_length=2048):\n",
    "    enc = tok_ft(text, return_tensors='pt', truncation=True, max_length=max_length)\n",
    "    with torch.no_grad():\n",
    "        logits = model_ft(**enc).logits\n",
    "        probs_t = F.softmax(logits, dim=-1).squeeze(0)\n",
    "    pred_id = int(logits.argmax(-1).item())\n",
    "\n",
    "    id2label_cfg = model_ft.config.id2label\n",
    "    if isinstance(id2label_cfg, dict):\n",
    "        label_map = {int(k) if isinstance(k, str) else k: v for k, v in id2label_cfg.items()}\n",
    "        label = label_map[pred_id]\n",
    "        probs = {label_map[i]: float(probs_t[i]) for i in range(len(probs_t))}\n",
    "    else:\n",
    "        label = id2label_cfg[pred_id]\n",
    "        probs = {id2label_cfg[i]: float(probs_t[i]) for i in range(len(probs_t))}\n",
    "    return {'label_id': pred_id, 'label': label, 'probs': probs, 'logits': logits.tolist()}\n",
    "\n",
    "classify_text('This text should look very plain and simple to read.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1988382a",
   "metadata": {},
   "source": [
    "## 10) Métricas en validation + Matriz de confusión"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02e986be",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = trainer.predict(tokenized['validation'])\n",
    "logits = pred.predictions\n",
    "y_true = pred.label_ids\n",
    "y_pred = np.argmax(logits, axis=1)\n",
    "\n",
    "m = logits.max(axis=1, keepdims=True)\n",
    "probs = np.exp(logits - m); probs = probs / probs.sum(axis=1, keepdims=True)\n",
    "p_pos = probs[:, 1]\n",
    "\n",
    "acc = accuracy_score(y_true, y_pred)\n",
    "p, r, f1, _ = precision_recall_fscore_support(y_true, y_pred, average='binary', zero_division=0)\n",
    "auc = None\n",
    "try:\n",
    "    if len(np.unique(y_true)) == 2:\n",
    "        auc = roc_auc_score(y_true, p_pos)\n",
    "except Exception:\n",
    "    pass\n",
    "\n",
    "print({'accuracy': round(acc,4), 'precision_tech': round(p,4), 'recall_tech': round(r,4), 'f1_tech': round(f1,4), 'auc_roc_tech': None if auc is None else round(auc,4)})\n",
    "\n",
    "cm = confusion_matrix(y_true, y_pred, labels=[0,1])\n",
    "cm_norm = cm.astype(float) / cm.sum(axis=1, keepdims=True)\n",
    "\n",
    "def plot_cm(cm, labels, title):\n",
    "    fig, ax = plt.subplots(figsize=(4.5,4))\n",
    "    im = ax.imshow(cm, interpolation='nearest')\n",
    "    ax.set_title(title)\n",
    "    ax.set_xlabel('Predicted label'); ax.set_ylabel('True label')\n",
    "    ax.set_xticks(np.arange(len(labels))); ax.set_yticks(np.arange(len(labels)))\n",
    "    ax.set_xticklabels(labels); ax.set_yticklabels(labels)\n",
    "    thresh = cm.max() / 2.0\n",
    "    for i in range(cm.shape[0]):\n",
    "        for j in range(cm.shape[1]):\n",
    "            val = cm[i,j]\n",
    "            ax.text(j, i, f\"{val:.3f}\" if isinstance(val,float) else f\"{val}\", ha='center', va='center', color='white' if val>thresh else 'black')\n",
    "    fig.tight_layout(); plt.show()\n",
    "\n",
    "id2label_cfg = model_ft.config.id2label\n",
    "if isinstance(id2label_cfg, dict):\n",
    "    id2label_plot = {int(k) if isinstance(k,str) else k: v for k, v in id2label_cfg.items()}\n",
    "else:\n",
    "    id2label_plot = {i: id2label_cfg[i] for i in range(len(id2label_cfg))}\n",
    "\n",
    "plot_cm(cm,      [id2label_plot[0], id2label_plot[1]], 'Confusion Matrix (counts)')\n",
    "plot_cm(cm_norm, [id2label_plot[0], id2label_plot[1]], 'Confusion Matrix (row-normalized)')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1f034bd",
   "metadata": {},
   "source": [
    "## 11) (Opcional) Subir al Hugging Face Hub (privado)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95358719",
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import create_repo, whoami\n",
    "\n",
    "me = whoami(); username = me['name']\n",
    "repo_id = f\"{username}/gemma3-270m-plaintech-ft\"\n",
    "print('Creando/subiendo a:', repo_id)\n",
    "create_repo(repo_id, private=True, exist_ok=True)\n",
    "\n",
    "tok_local = AutoTokenizer.from_pretrained('finetuned_gemma3_270m_cls', use_fast=True)\n",
    "mdl_local = Gemma3TextForSequenceClassification.from_pretrained('finetuned_gemma3_270m_cls')\n",
    "\n",
    "tok_local.push_to_hub(repo_id)\n",
    "mdl_local.push_to_hub(repo_id)\n",
    "print('Subido a HF Hub como:', repo_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a0012e4",
   "metadata": {},
   "source": [
    "## 12) (Opcional) Demo rápida en esta sesión"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "995ae8ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ipywidgets as widgets\n",
    "from IPython.display import display\n",
    "\n",
    "def classify_text_runtime(text: str, max_length=2048):\n",
    "    enc = tok_ft(text, return_tensors='pt', truncation=True, max_length=max_length)\n",
    "    with torch.no_grad():\n",
    "        logits = model_ft(**enc).logits\n",
    "        probs_t = torch.softmax(logits, dim=-1).squeeze(0)\n",
    "    pred_id = int(torch.argmax(probs_t).item())\n",
    "\n",
    "    id2label_cfg = model_ft.config.id2label\n",
    "    if isinstance(id2label_cfg, dict):\n",
    "        label_map = {int(k) if isinstance(k, str) else k: v for k, v in id2label_cfg.items()}\n",
    "        label = label_map[pred_id]\n",
    "        probs = {label_map[i]: float(probs_t[i]) for i in range(len(probs_t))}\n",
    "    else:\n",
    "        label = id2label_cfg[pred_id]\n",
    "        probs = {id2label_cfg[i]: float(probs_t[i]) for i in range(len(probs_t))}\n",
    "    return {'label': label, 'probs': {k: round(v,4) for k, v in probs.items()}}\n",
    "\n",
    "ta = widgets.Textarea(value='Type or paste your text here...', layout=widgets.Layout(width='100%', height='120px'))\n",
    "btn = widgets.Button(description='Classify')\n",
    "out = widgets.Output()\n",
    "\n",
    "def on_click(_):\n",
    "    out.clear_output()\n",
    "    with out:\n",
    "        res = classify_text_runtime(ta.value)\n",
    "        print('Prediction:', res['label'])\n",
    "        print('Probabilities:', res['probs'])\n",
    "\n",
    "display(ta, btn, out)\n",
    "btn.on_click(on_click)"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
